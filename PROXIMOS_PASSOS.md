# ğŸš€ Guia: PrÃ³ximos Passos

## âœ… O que foi criado atÃ© agora:

1. **`config.py`** - ConfiguraÃ§Ã£o centralizada com categorias
2. **`models.py`** - Modelos Pydantic para validaÃ§Ã£o
3. **`database.py`** - Gerenciador completo de BD SQLite
4. **`requirements.txt`** - DependÃªncias atualizadas

---

## ğŸ“‹ PrÃ³ximas Etapas (em ordem):

### 1ï¸âƒ£ **Instalar DependÃªncias** âš¡
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Testar o Banco de Dados** ğŸ§ª
```bash
python -c "from src.database import db; print('âœ… BD criado em: data/ml_crawler.db')"
```

### 3ï¸âƒ£ **Refatorar `scraper.py`** ğŸ”„
Modificar a funÃ§Ã£o `scrape_all_pages()` para:
- Receber a categoria como parÃ¢metro
- Salvar produtos no banco de dados
- Criar histÃ³rico de preÃ§os
- Retornar estatÃ­sticas da coleta

**MudanÃ§as principais:**
```python
def scrape_all_pages(categoria: str, url: str, max_pages: int):
    """
    Coleta produtos e salva no banco de dados
    """
    from .database import db
    from .models import Produto
    
    coleta_id = db.iniciar_coleta(categoria)
    total_novos = 0
    total_atualizados = 0
    
    for page in range(1, max_pages + 1):
        # ... scraping ...
        for produto_data in produtos:
            produto = Produto(
                nome=produto_data["nome"],
                preco=produto_data["preco"],
                link=produto_data["link"],
                categoria=categoria
            )
            
            # Verifica se jÃ¡ existe
            existente = db.obter_produto_por_link(produto.link)
            if existente:
                db.atualizar_preco(existente["id"], produto.preco)
                total_atualizados += 1
            else:
                db.adicionar_produto(produto)
                total_novos += 1
    
    db.finalizar_coleta(coleta_id, len(produtos), total_novos, total_atualizados, True)
```

### 4ï¸âƒ£ **Criar `tasks.py`** (Prefect) ğŸ“…
Criar tasks Prefect para:
- Scraping de cada categoria
- Flow que executa todas as categorias
- Schedule para rodar a cada 6 horas

### 5ï¸âƒ£ **Criar `analysis.py`** ğŸ“Š
Scripts para:
- Gerar estatÃ­sticas de produtos
- RelatÃ³rios por categoria
- Exportar dados para CSV/JSON

### 6ï¸âƒ£ **Atualizar `main.py`** ğŸ›ï¸
Adicionar commands para:
- Executar coleta manual de uma categoria
- Visualizar estatÃ­sticas
- Gerar relatÃ³rios

---

## ğŸ¯ Meta da Fase 1

Ter um sistema funcional que:
- âœ… Coleta produtos de 5 categorias
- âœ… Salva no banco de dados
- âœ… Cria histÃ³rico de preÃ§os
- âœ… Executa automaticamente a cada 6 horas
- âœ… Gera relatÃ³rios bÃ¡sicos

---

## ğŸ§ª Teste RÃ¡pido

Depois de instalar, rode:

```bash
# Testar configuraÃ§Ã£o
python -c "from src.config import CATEGORIAS; print(f'Categorias: {list(CATEGORIAS.keys())}')"

# Testar models
python -c "from src.models import Produto; p = Produto(nome='Teste', preco=100, link='https://test.com', categoria='teste'); print(p)"

# Testar banco
python -c "from src.database import db; print('âœ… Banco criado!')"
```

---

## ğŸ“ Checklist Hands-On

- [ ] Instalou dependÃªncias? (`pip install -r requirements.txt`)
- [ ] BD foi criado? (verifique em `data/ml_crawler.db`)
- [ ] Modelos estÃ£o funcionando?
- [ ] Database testes passam?
- [ ] Pronto para refatorar `scraper.py`?

---

**PrÃ³ximo: Refatore o `scraper.py` para integrar com o banco! ğŸš€**
